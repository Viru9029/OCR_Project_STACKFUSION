{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oBwtvVgTYccd"},"outputs":[],"source":["#Optical Character Recognition Using TensorFlow"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"aue7DF8rdT24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667897356608,"user_tz":-330,"elapsed":22346,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}},"outputId":"50338932-ba43-43c9-c72e-0610e90e53dc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Data Collection\n","# We’re building a character based OCR model in this article. For that we’ll be using 2 datasets.\n","\n","# The Standard MNIST 0–9 dataset by LECun et al. from tensorflow datasets\n","# The Kaggle A-Z dataset by Sachin Patel. link https://www.kaggle.com/datasets/sachinpatel21/az-handwritten-alphabets-in-csv-format?resource=download"],"metadata":{"id":"4cKGLvv0YmKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","from sklearn.preprocessing import LabelBinarizer\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"YE1OUZkHZV_g","executionInfo":{"status":"ok","timestamp":1667897372548,"user_tz":-330,"elapsed":3082,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Loading Datasets\n","from tensorflow.keras.datasets import mnist\n","\n","def load_mnist_dataset():\n","\n","  # load data from tensorflow framework\n","  ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data() \n","\n","  # Stacking train data and test data to form single array named data\n","  data = np.vstack([trainData, testData]) \n","\n","  # Vertical stacking labels of train and test set\n","  labels = np.hstack([trainLabels, testLabels]) \n","\n","  # return a 2-tuple of the MNIST data and labels\n","  return (data, labels)"],"metadata":{"id":"99D3ijgyY9Ml","executionInfo":{"status":"ok","timestamp":1667897372549,"user_tz":-330,"elapsed":10,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#we need one more function to load A-Z dataset\n","\n","def load_az_dataset(datasetPath):\n","\n","  # List for storing data\n","  data = []\n","  \n","  # List for storing labels\n","  labels = []\n","  \n","  for row in open(datasetPath): #Openfile and start reading each row\n","    #Split the row at every comma\n","    row = row.split(\",\")\n","    \n","    #row[0] contains label\n","    label = int(row[0])\n","    \n","    #Other all collumns contains pixel values make a saperate array for that\n","    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n","    \n","    #Reshaping image to 28 x 28 pixels\n","    image = image.reshape((28, 28))\n","    \n","    #append image to data\n","    data.append(image)\n","    \n","    #append label to labels\n","    labels.append(label)\n","    \n","  #Converting data to numpy array of type float32\n","  data = np.array(data, dtype='float32')\n","  \n","  #Converting labels to type int\n","  labels = np.array(labels, dtype=\"int\")\n","  \n","  return (data, labels)"],"metadata":{"id":"kEXHZ-XKZRLR","executionInfo":{"status":"ok","timestamp":1667897373563,"user_tz":-330,"elapsed":4,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#let’s call the function and our dataset is ready\n","(digitsData, digitsLabels) = load_mnist_dataset()\n","\n","(azData, azLabels) = load_az_dataset('/content/drive/MyDrive/Colab Notebooks/A_Z Handwritten_Data.csv')"],"metadata":{"id":"XO_nU-pAZiq6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667897443775,"user_tz":-330,"elapsed":68081,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}},"outputId":"a20874c4-629a-4c63-da99-9050b564bc9f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["#Combining datasets and dataset preparation\n","#Now we need to combine both the datasets for feeding into model. This can be done with few lines of code.\n","# the MNIST dataset occupies the labels 0-9, so let's add 10 to every A-Z label to ensure the A-Z characters are not incorrectly labeled \n","\n","azLabels += 10\n","\n","# stack the A-Z data and labels with the MNIST digits data and labels\n","\n","data = np.vstack([azData, digitsData])\n","labels = np.hstack([azLabels, digitsLabels])\n","\n","# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n","# However, the architecture we're using is designed for 32x32 images,\n","# So we need to resize them to 32x32\n","\n","data = [cv2.resize(image, (32, 32)) for image in data]\n","data = np.array(data, dtype=\"float32\")\n","\n","# add a channel dimension to every image in the dataset and scale the\n","# pixel intensities of the images from [0, 255] down to [0, 1]\n","\n","data = np.expand_dims(data, axis=-1)\n","data /= 255.0"],"metadata":{"id":"waC11VvFZi72","executionInfo":{"status":"ok","timestamp":1667897454447,"user_tz":-330,"elapsed":10703,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Here we’re adding 10 to each label in a-z dataset because we are going to stack them up with mnist dataset. Then we are stacking data and labels then our model architecture needs images into 32 x 32 pixels so we’ve to resize it further we are adding a channel dimension to every image and scale the pixel intensities from [0–255] down to [0–1]\n","#Now we have to convert labels from integer to vector for ease in model fitting and see the count the weights of each character in the dataset and also count the classweights for each label.\n","\n","le = LabelBinarizer()\n","labels = le.fit_transform(labels)\n","\n","counts = labels.sum(axis=0)\n","\n","# account for skew in the labeled data\n","classTotals = labels.sum(axis=0)\n","classWeight = {}\n","\n","# loop over all classes and calculate the class weight\n","for i in range(0, len(classTotals)):\n","  classWeight[i] = classTotals.max() / classTotals[i]\n","  \n","# partition the data into training and testing splits using 80% of\n","# the data for training and the remaining 20% for testing\n","(trainX, testX, trainY, testY) = train_test_split(data,\n","\tlabels, test_size=0.20, stratify=labels, random_state=42)"],"metadata":{"id":"T_guNQYIZjA_","executionInfo":{"status":"ok","timestamp":1667897469686,"user_tz":-330,"elapsed":15270,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Performing Data Augmentation\n","#We can improve the results of our ResNet classifier by augmenting the input data for training using ImageDataGenerator. We are using various scaling rotations, scaling the size, horizontal translations, vertical translations, and tilts in the images. Here is a block of code through which we perform data augmentation.\n","# construct the image generator for data augmentation\n","\n","aug = ImageDataGenerator(\n","rotation_range=10,\n","zoom_range=0.05,\n","width_shift_range=0.1,\n","height_shift_range=0.1,\n","shear_range=0.15,\n","horizontal_flip=False,\n","fill_mode=\"nearest\")"],"metadata":{"id":"yyCz5ZEjZjDl","executionInfo":{"status":"ok","timestamp":1667897469687,"user_tz":-330,"elapsed":35,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Now our data is ready so let’s build the heart of our Project i.e ResNet architecture.\n","#Building ResNet Architecture\n","import keras\n","from keras.layers import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import AveragePooling2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.convolutional import ZeroPadding2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Dense\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import add\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","class ResNet:\n","\t@staticmethod\n","\tdef residual_module(data, K, stride, chanDim, red=False,\n","\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n","\t\t# the shortcut branch of the ResNet module should be\n","\t\t# initialize as the input (identity) data\n","\t\tshortcut = data\n","\n","\t\t# the first block of the ResNet module are the 1x1 CONVs\n","\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(data)\n","\t\tact1 = Activation(\"relu\")(bn1)\n","\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act1)\n","\n","\t\t# the second block of the ResNet module are the 3x3 CONVs\n","\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(conv1)\n","\t\tact2 = Activation(\"relu\")(bn2)\n","\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n","\t\t\tpadding=\"same\", use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act2)\n","\n","\t\t# the third block of the ResNet module is another set of 1x1\n","\t\t# CONVs\n","\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(conv2)\n","\t\tact3 = Activation(\"relu\")(bn3)\n","\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n","\t\t\tkernel_regularizer=l2(reg))(act3)\n","\n","\t\t# if we are to reduce the spatial size, apply a CONV layer to\n","\t\t# the shortcut\n","\t\tif red:\n","\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n","\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n","\n","\t\t# add together the shortcut and the final CONV\n","\t\tx = add([conv3, shortcut])\n","\n","\t\t# return the addition as the output of the ResNet module\n","\t\treturn x\n","\n","\t@staticmethod\n","\tdef build(width, height, depth, classes, stages, filters,\n","\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n","\t\t# initialize the input shape to be \"channels last\" and the\n","\t\t# channels dimension itself\n","\t\tinputShape = (height, width, depth)\n","\t\tchanDim = -1\n","\n","\t\t# if we are using \"channels first\", update the input shape\n","\t\t# and channels dimension\n","\t\tif K.image_data_format() == \"channels_first\":\n","\t\t\tinputShape = (depth, height, width)\n","\t\t\tchanDim = 1\n","\n","\t\t# set the input and apply BN\n","\t\tinputs = Input(shape=inputShape)\n","\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(inputs)\n","\n","\t\t# check if we are utilizing the CIFAR dataset\n","\t\tif dataset == \"cifar\":\n","\t\t\t# apply a single CONV layer\n","\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n","\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n","\n","\t\t# check to see if we are using the Tiny ImageNet dataset\n","\t\telif dataset == \"tiny_imagenet\":\n","\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n","\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n","\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n","\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\t\tmomentum=bnMom)(x)\n","\t\t\tx = Activation(\"relu\")(x)\n","\t\t\tx = ZeroPadding2D((1, 1))(x)\n","\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","\t\t# loop over the number of stages\n","\t\tfor i in range(0, len(stages)):\n","\t\t\t# initialize the stride, then apply a residual module\n","\t\t\t# used to reduce the spatial size of the input volume\n","\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n","\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n","\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n","\n","\t\t\t# loop over the number of layers in the stage\n","\t\t\tfor j in range(0, stages[i] - 1):\n","\t\t\t\t# apply a ResNet module\n","\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n","\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n","\n","\t\t# apply BN => ACT => POOL\n","\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","\t\t\tmomentum=bnMom)(x)\n","\t\tx = Activation(\"relu\")(x)\n","\t\tx = AveragePooling2D((8, 8))(x)\n","\n","\t\t# softmax classifier\n","\t\tx = Flatten()(x)\n","\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n","\t\tx = Activation(\"softmax\")(x)\n","\n","\t\t# create the model\n","\t\tmodel = Model(inputs, x, name=\"resnet\")\n","\n","\t\t# return the constructed network architecture\n","\t\treturn model"],"metadata":{"id":"iRwwDvKRZjGA","executionInfo":{"status":"ok","timestamp":1667897469688,"user_tz":-330,"elapsed":34,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Compiling model\n","EPOCHS = 50\n","INIT_LR = 1e-1\n","BS = 128"],"metadata":{"id":"xlTaGHaKZjIh","executionInfo":{"status":"ok","timestamp":1667897469689,"user_tz":-330,"elapsed":34,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#let’s fit the model.\n","from keras.optimizers import SGD\n","opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n","\n","model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n","(64, 64, 128, 256), reg=0.0005)\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n","\n","H = model.fit(\n","aug.flow(trainX, trainY, batch_size=BS),\n","validation_data=(testX, testY),\n","steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n","class_weight=classWeight,\n","verbose=1)"],"metadata":{"id":"rICriYfOZjKq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e5f9a2e-3fc9-47a5-9c6a-ff4887701429","executionInfo":{"status":"ok","timestamp":1667903788113,"user_tz":-330,"elapsed":6309812,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","2765/2765 [==============================] - 138s 46ms/step - loss: 1.8273 - accuracy: 0.8732 - val_loss: 0.5415 - val_accuracy: 0.8879\n","Epoch 2/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.9184 - accuracy: 0.9307 - val_loss: 0.5215 - val_accuracy: 0.8983\n","Epoch 3/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.8414 - accuracy: 0.9373 - val_loss: 0.5447 - val_accuracy: 0.8840\n","Epoch 4/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.7927 - accuracy: 0.9407 - val_loss: 0.4759 - val_accuracy: 0.9163\n","Epoch 5/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.7673 - accuracy: 0.9433 - val_loss: 0.4546 - val_accuracy: 0.9290\n","Epoch 6/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.7477 - accuracy: 0.9460 - val_loss: 0.4478 - val_accuracy: 0.9331\n","Epoch 7/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.7353 - accuracy: 0.9473 - val_loss: 0.4664 - val_accuracy: 0.9251\n","Epoch 8/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.7167 - accuracy: 0.9498 - val_loss: 0.4270 - val_accuracy: 0.9475\n","Epoch 9/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.7043 - accuracy: 0.9510 - val_loss: 0.4286 - val_accuracy: 0.9451\n","Epoch 10/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.6991 - accuracy: 0.9518 - val_loss: 0.4025 - val_accuracy: 0.9595\n","Epoch 11/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.6891 - accuracy: 0.9528 - val_loss: 0.4123 - val_accuracy: 0.9547\n","Epoch 12/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6784 - accuracy: 0.9542 - val_loss: 0.4036 - val_accuracy: 0.9577\n","Epoch 13/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.6716 - accuracy: 0.9542 - val_loss: 0.3876 - val_accuracy: 0.9647\n","Epoch 14/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6659 - accuracy: 0.9556 - val_loss: 0.3963 - val_accuracy: 0.9592\n","Epoch 15/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6608 - accuracy: 0.9566 - val_loss: 0.3848 - val_accuracy: 0.9651\n","Epoch 16/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.6574 - accuracy: 0.9572 - val_loss: 0.4093 - val_accuracy: 0.9523\n","Epoch 17/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.6487 - accuracy: 0.9579 - val_loss: 0.3868 - val_accuracy: 0.9633\n","Epoch 18/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6451 - accuracy: 0.9582 - val_loss: 0.3888 - val_accuracy: 0.9619\n","Epoch 19/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.6394 - accuracy: 0.9588 - val_loss: 0.3820 - val_accuracy: 0.9657\n","Epoch 20/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6392 - accuracy: 0.9598 - val_loss: 0.3885 - val_accuracy: 0.9629\n","Epoch 21/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6314 - accuracy: 0.9603 - val_loss: 0.3806 - val_accuracy: 0.9652\n","Epoch 22/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.6304 - accuracy: 0.9610 - val_loss: 0.3801 - val_accuracy: 0.9665\n","Epoch 23/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6241 - accuracy: 0.9616 - val_loss: 0.3874 - val_accuracy: 0.9653\n","Epoch 24/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6213 - accuracy: 0.9619 - val_loss: 0.3786 - val_accuracy: 0.9667\n","Epoch 25/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6165 - accuracy: 0.9625 - val_loss: 0.3816 - val_accuracy: 0.9647\n","Epoch 26/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6134 - accuracy: 0.9630 - val_loss: 0.3826 - val_accuracy: 0.9629\n","Epoch 27/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.6109 - accuracy: 0.9637 - val_loss: 0.3793 - val_accuracy: 0.9646\n","Epoch 28/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6046 - accuracy: 0.9636 - val_loss: 0.3761 - val_accuracy: 0.9673\n","Epoch 29/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.6067 - accuracy: 0.9638 - val_loss: 0.3739 - val_accuracy: 0.9670\n","Epoch 30/50\n","2765/2765 [==============================] - 127s 46ms/step - loss: 0.5997 - accuracy: 0.9644 - val_loss: 0.3844 - val_accuracy: 0.9664\n","Epoch 31/50\n","2765/2765 [==============================] - 125s 45ms/step - loss: 0.5972 - accuracy: 0.9651 - val_loss: 0.3735 - val_accuracy: 0.9678\n","Epoch 32/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5973 - accuracy: 0.9649 - val_loss: 0.3952 - val_accuracy: 0.9549\n","Epoch 33/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5928 - accuracy: 0.9654 - val_loss: 0.3826 - val_accuracy: 0.9663\n","Epoch 34/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5880 - accuracy: 0.9663 - val_loss: 0.3858 - val_accuracy: 0.9664\n","Epoch 35/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5893 - accuracy: 0.9663 - val_loss: 0.3770 - val_accuracy: 0.9668\n","Epoch 36/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5884 - accuracy: 0.9662 - val_loss: 0.3834 - val_accuracy: 0.9664\n","Epoch 37/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5842 - accuracy: 0.9670 - val_loss: 0.3717 - val_accuracy: 0.9676\n","Epoch 38/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5817 - accuracy: 0.9668 - val_loss: 0.3720 - val_accuracy: 0.9670\n","Epoch 39/50\n","2765/2765 [==============================] - 128s 46ms/step - loss: 0.5742 - accuracy: 0.9678 - val_loss: 0.4111 - val_accuracy: 0.9478\n","Epoch 40/50\n","2765/2765 [==============================] - 127s 46ms/step - loss: 0.5758 - accuracy: 0.9679 - val_loss: 0.4167 - val_accuracy: 0.9629\n","Epoch 41/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5735 - accuracy: 0.9680 - val_loss: 0.3691 - val_accuracy: 0.9684\n","Epoch 42/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5759 - accuracy: 0.9680 - val_loss: 0.3817 - val_accuracy: 0.9614\n","Epoch 43/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5698 - accuracy: 0.9689 - val_loss: 0.3642 - val_accuracy: 0.9694\n","Epoch 44/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5693 - accuracy: 0.9686 - val_loss: 0.3680 - val_accuracy: 0.9679\n","Epoch 45/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5684 - accuracy: 0.9687 - val_loss: 0.3637 - val_accuracy: 0.9703\n","Epoch 46/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5630 - accuracy: 0.9698 - val_loss: 0.3755 - val_accuracy: 0.9675\n","Epoch 47/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5668 - accuracy: 0.9692 - val_loss: 0.3642 - val_accuracy: 0.9697\n","Epoch 48/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5633 - accuracy: 0.9693 - val_loss: 0.3656 - val_accuracy: 0.9690\n","Epoch 49/50\n","2765/2765 [==============================] - 126s 45ms/step - loss: 0.5624 - accuracy: 0.9694 - val_loss: 0.3667 - val_accuracy: 0.9679\n","Epoch 50/50\n","2765/2765 [==============================] - 126s 46ms/step - loss: 0.5588 - accuracy: 0.9699 - val_loss: 0.3600 - val_accuracy: 0.9709\n"]}]},{"cell_type":"code","source":["#Model Evaluation\n","from sklearn.metrics import classification_report\n","labelNames = \"0123456789\"\n","\n","labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","\n","labelNames = [l for l in labelNames]\n","\n","predictions = model.predict(testX, batch_size=BS)\n","\n","print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))\n"],"metadata":{"id":"SZ-UyVm8a8CM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667903792408,"user_tz":-330,"elapsed":4303,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}},"outputId":"1001288b-b0ee-4aea-ddd1-601a79073ab1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["692/692 [==============================] - 4s 5ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.46      0.55      1381\n","           1       0.98      0.99      0.98      1575\n","           2       0.96      0.94      0.95      1398\n","           3       0.99      0.99      0.99      1428\n","           4       0.94      0.98      0.96      1365\n","           5       0.85      0.92      0.88      1263\n","           6       0.97      0.98      0.97      1375\n","           7       0.98      0.99      0.99      1459\n","           8       0.98      0.99      0.98      1365\n","           9       0.99      0.99      0.99      1392\n","           A       1.00      0.99      1.00      2774\n","           B       0.99      0.99      0.99      1734\n","           C       0.99      0.98      0.99      4682\n","           D       0.90      0.99      0.94      2027\n","           E       0.99      0.99      0.99      2288\n","           F       0.97      0.99      0.98       232\n","           G       0.97      0.96      0.97      1152\n","           H       0.98      0.98      0.98      1444\n","           I       0.97      0.99      0.98       224\n","           J       0.98      0.98      0.98      1699\n","           K       0.98      0.99      0.98      1121\n","           L       0.97      0.99      0.98      2317\n","           M       0.99      1.00      0.99      2467\n","           N       0.99      0.99      0.99      3802\n","           O       0.94      0.96      0.95     11565\n","           P       1.00      0.99      0.99      3868\n","           Q       0.97      0.99      0.98      1162\n","           R       0.99      0.99      0.99      2313\n","           S       0.99      0.98      0.98      9684\n","           T       0.99      0.99      0.99      4499\n","           U       0.99      0.98      0.99      5802\n","           V       0.93      1.00      0.96       836\n","           W       0.99      0.99      0.99      2157\n","           X       0.99      1.00      0.99      1254\n","           Y       0.99      0.95      0.97      2172\n","           Z       0.93      0.97      0.95      1215\n","\n","    accuracy                           0.97     88491\n","   macro avg       0.96      0.97      0.96     88491\n","weighted avg       0.97      0.97      0.97     88491\n","\n"]}]},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/Colab Notebooks/OCR_Resnet3.h5',save_format=\".h5\")"],"metadata":{"id":"kCibIHR_a8Eu","executionInfo":{"status":"ok","timestamp":1667903792409,"user_tz":-330,"elapsed":15,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#let’s check how our model is performing with actual images.\n","from imutils import build_montages\n","from google.colab.patches import cv2_imshow\n","images = []\n","output = ''\n","# randomly select a few testing characters\n","for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n","  probs = model.predict(testX[np.newaxis, i])\n","  prediction = probs.argmax(axis=1)\n","  label = labelNames[prediction[0]]\n","  output+=label\n","  image = (testX[i] * 255).astype(\"uint8\")\n","  color = (0, 255, 0)\n","  if prediction[0] != np.argmax(testY[i]):\n","    color = (0, 0, 255)\n","    image = cv2.merge([image] * 3)\n","    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n","    cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n","    color, 2)\n","    images.append(image)\n","  \n","montage = build_montages(images, (96, 96), (7, 7))[0]\n","cv2_imshow(montage)\n","cv2.waitKey(0)"],"metadata":{"id":"aGW7mRJ8a8G9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1667903864340,"user_tz":-330,"elapsed":3076,"user":{"displayName":"Humanoid Assistant","userId":"12562886366433044982"}},"outputId":"94958a89-2307-4cb7-9eb4-ce3106e26efd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=672x672 at 0x7F765617E090>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["-1"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[],"metadata":{"id":"G2afyChWa8JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YgE6Q9vHa8Lj"},"execution_count":null,"outputs":[]}]}